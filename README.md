# [汽车论坛消费者用车体验内容的判别与标注](https://www.datafountain.cn/competitions/365)

最终排名：15/900，记录一下预训练模型的简单使用。

## 探索性数据分析：

正负样本比例：1：3

文本长度分布

|       | len_title   | len_content |
| :---- | :---------- | :---------- |
| count | 3993.000000 | 3993.000000 |
| mean  | 24.929627   | 199.721262  |
| std   | 30.395124   | 520.193358  |
| min   | 2.000000    | 0.000000    |
| 25%   | 10.000000   | 13.000000   |
| 50%   | 17.000000   | 33.000000   |
| 75%   | 27.000000   | 102.000000  |
| max   | 255.000000  | 7560.000000 |

观察特别短和特别长的标题及内容，并对数据进行清洗。

## 预训练模型的使用

使用了[民事、刑事、百科中文预训练语言模型仓库](https://github.com/thunlp/OpenCLaP)，但效果几乎没有提升，可能是因为汽车评论数据与民事、刑事关系不大。

### 常见预训练模型的区别

BERT、RoBERT、XLNet、BERT-WWM

### 模型融合

1. logits相加取最大
2. 多个模型投票：提升不到一个点

### 调参

* 学习率

  1e-5、2e-5、3e-5、5e-5

* batch_size

  16、32

* max_seq_length

  128、196

## trick

1. 数据
   * 数据增强：同义词、近义词替换，几乎没有提升，可能是信息重复。
   * 取title + content部分内容：前若干字与后若干字的组合、摘要提取
2. 预处理
   * 通过网络将文本长度压缩：没有尝试
3. 后处理
   * 分类阈值调整：提升一个点左右。
   * 连接其它网络层：如CNN等，但效果没用提升。
   * 通过观察数据与badcase自定义规则进行处理

4. 冻结预训练模型的某些层与连接最后几层的输出

## Reference

[1] [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583?context=cs.CL)

